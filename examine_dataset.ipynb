{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dataset length: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# We will examing the shakespear dataset here\n",
    "\n",
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print('total dataset length:', len(data))\n",
    "\n",
    "# print the first 100 characters\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# As previously, let us find out all the unique characters in the dataset\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(str(vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42]\n",
      "hello, world\n",
      "50257\n",
      "[31373, 11, 995]\n",
      "hello, world\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the dataset: converting the text to integers to feed into the embedding matrix\n",
    "\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[c] for c in l])\n",
    "\n",
    "# Print some encoding and decomding\n",
    "print(encode('hello, world'))\n",
    "print(decode(encode('hello, world')))\n",
    "\n",
    "# this is very simple compared to other implementationms, i.e., Google's SentencePiece\n",
    "# which is a sub-word level tokenizer instead of a character level tokenizer like ours.\n",
    "# we have used in train.bin and val.bin, we have used tiktoken's gpt2 bpe tokenizer.\n",
    "\n",
    "# let us see what this encoder looks like:\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.n_vocab) # prints total vocabulary size, which for ours is 65 and for them it is 50257.as_integer_ratio\n",
    "print(enc.encode('hello, world'))\n",
    "print(enc.decode(enc.encode('hello, world')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "torch.Size([338025]) torch.int64\n",
      "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
      "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
      "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
      "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
      "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
      "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
      "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
      "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
      "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataset = torch.tensor(encode(data),dtype=torch.long) # encode all of our shakesphere dataset\n",
    "print(dataset.shape,dataset.dtype)\n",
    "print(dataset[:100])\n",
    "\n",
    "dataset_bpe = torch.tensor(enc.encode(data),dtype=torch.long) # encode all of our shakesphere dataset\n",
    "print(dataset_bpe.shape,dataset_bpe.dtype)\n",
    "print(dataset_bpe[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the training and validation datasets in a 90/10 split\n",
    "\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "train_dataset, val_dataset = dataset[:train_size], dataset[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # this is the sequence length of the input to the model\n",
    "train_dataset[:block_size+1] # the +1 has a special purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_dataset[:block_size] # input to dataset\n",
    "y = train_dataset[1:block_size+1] # input plus the next character in the block\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "\n",
    "# The idea behind this is to make the transformer model\n",
    "# learn to see context as llittle as 1 character and as much as block_size characters\n",
    "# when trying to make the next prediction.\n",
    "# This can be a representation of the 'time' dimension, as analogous to the case of wavenets for speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "when input is tensor([24]) the target is 43\n",
      "when input is tensor([24, 43]) the target is 58\n",
      "when input is tensor([24, 43, 58]) the target is 5\n",
      "when input is tensor([24, 43, 58,  5]) the target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "when input is tensor([44]) the target is 53\n",
      "when input is tensor([44, 53]) the target is 56\n",
      "when input is tensor([44, 53, 56]) the target is 1\n",
      "when input is tensor([44, 53, 56,  1]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52]) the target is 58\n",
      "when input is tensor([52, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58]) the target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "when input is tensor([25]) the target is 17\n",
      "when input is tensor([25, 17]) the target is 27\n",
      "when input is tensor([25, 17, 27]) the target is 10\n",
      "when input is tensor([25, 17, 27, 10]) the target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "# Now generalizing this to batch dimensions to feed to the GPU.\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batchf rom dataset\n",
    "    data = train_dataset if split == 'train' else val_dataset\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size): # loop over batches\n",
    "    for t in range(block_size): # loop over time, or sequence in our case\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramModel(\n",
      "  (token_embedding_table): Embedding(65, 65)\n",
      ")\n",
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# Let us feed this into a simple model: the Bigram model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # this is a square embedding matrix\n",
    "        # This is a token embedding table which is the C matrix.\n",
    "        # When you pass idx to it, it will return a row corresponding to the index.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx represents a batch of sequences (B), targets represents the time token (T)\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        # logits is a tensor of shape (B,T,C) where C is the vocab size, or the channel size.\n",
    "        # \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C) # convert to 2D array using view\n",
    "            targets = targets.view(B*T) # convert to 1D array using view\n",
    "            loss = F.cross_entropy(logits,targets) # we will use the cross entropy loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx,max_new_tokens): # generates new character for the predicted character\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits,loss = self(idx)\n",
    "            # only consider the last time instance in the BXTXC logits matrix\n",
    "            logits = logits[:,-1,:]  # this makes it a BXC matrix as the last time instance is considered\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits,dim = -1) # BXC matrix\n",
    "            # sample from the distritbution\n",
    "            idx_next = torch.multinomial(probs,num_samples = 1) # now this is a Bx1 matrix, one prediction per batch\n",
    "            # append the character to the running sequence\n",
    "            idx = torch.cat([idx,idx_next],dim = 1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "model = BigramModel(vocab_size)\n",
    "print(model)\n",
    "#out = model(xb,yb)\n",
    "logits, loss = model(xb,yb) # this should not work as of now. because when a multidimensional\n",
    "# tensor is passed to the cross entropy loss, it expects a BxCxT tensor, where C is the number of classes,\n",
    "# while our tensor is a BxTxC tensor. So we need to permute the dimensions of the tensor.\n",
    "print(logits.shape)\n",
    "\n",
    "print(loss) # which should be approximately -log_n(1/65), the negative log likelihood of a random guess.\n",
    "\n",
    "idx = torch.zeros((1,1),dtype=torch.long)\n",
    "print(decode(model.generate(idx,max_new_tokens = 100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small check to see what device pytorch is using\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is 2.2298288345336914\n",
      "loss at iteration 1000 is 2.3685214519500732\n",
      "loss at iteration 2000 is 2.6540417671203613\n",
      "loss at iteration 3000 is 2.360241651535034\n",
      "loss at iteration 4000 is 2.616257667541504\n",
      "loss at iteration 5000 is 2.739475965499878\n",
      "loss at iteration 6000 is 3.0783445835113525\n",
      "loss at iteration 7000 is 2.3750882148742676\n",
      "loss at iteration 8000 is 2.3131730556488037\n",
      "loss at iteration 9000 is 2.4603514671325684\n",
      "loss on validation set is 2.288727045059204\n",
      "\n",
      "Tontcow qure outhatr hy ERWI an. ad as!\n",
      "Fowe wise selk fath!\n",
      "ssttst t ses aveeacofa\n",
      "NILAnd the\n",
      "KIOr \n"
     ]
    }
   ],
   "source": [
    "# Let us train this bigram model\n",
    "\n",
    "max_iters = 10000\n",
    "lr = 10\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # get a batch\n",
    "    xb,yb=get_batch('train')\n",
    "    # get predictions\n",
    "    logits,loss = model(xb,yb)\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            p -= lr * p.grad\n",
    "        model.zero_grad()\n",
    "    # print loss\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"loss at iteration {i} is {loss.item()}\")\n",
    "\n",
    "# Let us see how the model performs on the validation set\n",
    "xv,yv = get_batch('val')\n",
    "logits_val,loss_val = model(xv,yv)\n",
    "print(f\"loss on validation set is {loss_val.item()}\")\n",
    "idx = torch.zeros((1,1),dtype=torch.long)\n",
    "print(decode(model.generate(idx,max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is 2.407052516937256\n",
      "loss at iteration 1000 is 2.5036914348602295\n",
      "loss at iteration 2000 is 2.4258666038513184\n",
      "loss at iteration 3000 is 2.425645589828491\n",
      "loss at iteration 4000 is 2.4620988368988037\n",
      "loss at iteration 5000 is 2.4708340167999268\n",
      "loss at iteration 6000 is 2.455707550048828\n",
      "loss at iteration 7000 is 2.3786418437957764\n",
      "loss at iteration 8000 is 2.49593186378479\n",
      "loss at iteration 9000 is 2.4702494144439697\n",
      "\n",
      "hid m y, pucear malongeflesamy h he ce.\n",
      "MNGRKI\n",
      "Wh! gieyo t.\n",
      "CAN: ale an!\n",
      "Barke t CKillcichouburis\n",
      "\n",
      "T\n",
      "loss on validation set is 2.452451705932617\n"
     ]
    }
   ],
   "source": [
    "# Another way to optimize is using the Adam optimizer\n",
    "lr = 1e-3 # learning rate for Adam is usually smaller than GD which was the one we used above\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=lr) # typically the lr is 3e-4 for bigger models\n",
    "batch_size = 128\n",
    "max_iters = 10000\n",
    "for i in range(max_iters):\n",
    "    # get a batch\n",
    "    xb,yb = get_batch('train')\n",
    "    # get predictions\n",
    "    logits,loss = model(xb,yb)\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "    # perform the update\n",
    "    optimizer.step() # same as our for loop\n",
    "    # print loss\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"loss at iteration {i} is {loss.item()}\")\n",
    "\n",
    "# print some outputs\n",
    "idx = torch.zeros((1,1),dtype=torch.long)\n",
    "print(decode(model.generate(idx,max_new_tokens=100)[0].tolist()))    \n",
    "# Get validation loss\n",
    "xv,yv = get_batch('val')\n",
    "logits_val,loss_val = model(xv,yv)\n",
    "print(f\"loss on validation set is {loss_val.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98cbb787251fac8b09d58c88d44135973ceca7df75589457d3db0159f5eed1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
