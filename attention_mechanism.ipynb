{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2]) torch.Size([4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# some theory on attention mechanism\n",
    "\n",
    "import torch\n",
    "\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C) # this corresponds to the BTC definition we see in our logits matrix. B is batch, T is time/sequence, C is characters/classes\n",
    "print(x.shape)\n",
    "\n",
    "# we would like these tokens to talk to each other to gather their autocorrelative concepts.\n",
    "# Currently, we do not have this functionality enabled in the Bigram model or any other language model \n",
    "# we have developed.\n",
    "\n",
    "# The simplest way to do this by averaging over the preceeding elements in the sequence/time dimension.\n",
    "# this is very rudimentary and weak, but we can try this way.\n",
    "# This can be achieved in the following way:\n",
    "xbow = torch.zeros(B,T,C) # bow is bag of words, a jargon in NLP denoting some samples of words averaged over\n",
    "for b in range(B): #looping over batch dimension\n",
    "    for t in range(T): # looping over sequences\n",
    "        xprev = x[b,:t+1,:] # +1 is how python works\n",
    "        xbow[b,t] = xprev.mean(dim=0)\n",
    "\n",
    "print(xbow.shape,x[:,:2,:].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([3, 2]) torch.Size([3, 2])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# While the average somehow works, it is inefficient. We can do this\n",
    "# better using matrix multiplication\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "C = a @ b\n",
    "print(a.shape,b.shape,C.shape)\n",
    "print(a,b,C,sep='\\n')\n",
    "# Shows that if a contains only ones, then the matrix multiplication is essentially\n",
    "# a sum over columns of b. \n",
    "# However, the matrix is summing over all the elements, but we want to sum over only\n",
    "# the preceeding elements. We can do this by using a mask.\n",
    "# let us create a mask for this purpose\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "C  = a @ b\n",
    "print(a,b,C,sep='\\n')\n",
    "# Now we see that there is some consideration only for the preceeding elements.\n",
    "# In order to average, we need to sum over the number of elements considered in the average.\n",
    "a_count = torch.tril(torch.ones(3,3)).sum(dim=1)\n",
    "print(a_count)\n",
    "# Therefore, we get the following\n",
    "C = (a @ b) / a_count[:,None]\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us now implement this for the xbow matrix\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(dim=1)[:,None]\n",
    "wei\n",
    "xbow2 = wei @ x # which is (B,T,T) @ (B,T,C) matrix. \n",
    "# In pytorch, the B will be non interactive, (T,T) will multiply with (T,C) to give (T,C)\n",
    "# giving us a (B,T,C) matrix\n",
    "print(xbow2.shape)\n",
    "\n",
    "# Therefore, the attention mechanism is a matrix multiplication \n",
    "# that achieves a weighted sum of the preceeding elements in the sequence,\n",
    "# The weights we have used thus far have the same value for all the elements\n",
    "# Therefore all the elements are weighted equally. This means that \n",
    "# all the previous characters have equal effect on the next character prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is another way to assemble the wei matrix.\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "# This fills up the matrix with negative infinite in the upper triangular part\n",
    "print(wei)\n",
    "# If we now take a softmax of this matrix:\n",
    "wei = torch.nn.functional.softmax(wei,dim=1)\n",
    "# we get the following effect:\n",
    "print(wei)\n",
    "# This is the same, except now, we have used a softmax to get the same effect\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow,xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Self attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C) # this corresponds to the BTC definition we see in our logits matrix. B is batch, T is time/sequence, C is characters/classes\n",
    "\n",
    "# wei is simply averaging uniformly, but we want it to be more dynamic way.\n",
    "# Self attention is a way to do this.\n",
    "# Every single node will emit two vectors, a query and key.\n",
    "# query: what am I looking for?\n",
    "# key: what do I contain?\n",
    "\n",
    "# let us see a single Head perform self attention\n",
    "head_size = 16\n",
    "key = torch.nn.Linear(C,head_size,bias=False)\n",
    "query = torch.nn.Linear(C,head_size,bias=False)\n",
    "value = torch.nn.Linear(C,head_size,bias=False)\n",
    "k = key(x) # B,T,head_size\n",
    "q = query(x) # B,T,head_size\n",
    "wei = q @ k.transpose(-2,-1) # B,T,n_heads @ B,n_heads,T = B,T,T\n",
    "\n",
    "# We can now apply the masking to this instead.\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei,dim=-1)\n",
    "#print(wei)\n",
    "#out = wei @ x\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(wei[0]) # B,T,head_size\n",
    "\n",
    "# So self attention is a way to get a weighted sum of the preceeding elements\n",
    "# where the weights are determined by the query and key vectors.\n",
    "# and the query and key vectors are learned from the data through training.\n",
    "\n",
    "# Attention seems to be a communication mechanism between the nodes in the graph.\n",
    "# In our case, the communication is done in an autoregressive manner, where future\n",
    "# nodes are not allowed to communicate with the past nodes.\n",
    "\n",
    "# As the operation is done batch-wise, batches do not communicate.\n",
    "\n",
    "# We can also delete the tril tensor from being used by removing it.\n",
    "# This allows all the nodes to talk to each other. This is called an 'encoder block'.\n",
    "\n",
    "# In the decoder block, the tril tensor will have.\n",
    "\n",
    "# Self attention means the key, value and query are coming from the same source.\n",
    "# Cross attention means the key, value and query are coming from different sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9946) tensor(0.9790) tensor(15.1783)\n",
      "tensor(0.9486)\n"
     ]
    }
   ],
   "source": [
    "# Scaled dot product attention\n",
    "# This is a normalization applied to the wei matrix.\n",
    "\n",
    "# This is because:\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) # B,T,n_heads @ B,n_heads,T = B,T,T\n",
    "print(k.var(),q.var(),wei.var())\n",
    "# We get an enormous variance in the wei matrix. This makes it unstable during training.\n",
    "# We can normalize this by dividing by the square root of the head_size.\n",
    "wei = wei / (head_size**0.5)\n",
    "print(wei.var())\n",
    "# This is important because as wei is fed into softmax, it will be exponentiated.\n",
    "# this will converge the wei to a one hot vector as the softmax sharpens the distribution towards\n",
    "# the maximum.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98cbb787251fac8b09d58c88d44135973ceca7df75589457d3db0159f5eed1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
